{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[RL강의] DQN_extentions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylee33/RL_Lecture/blob/master/%5BRL%EA%B0%95%EC%9D%98%5D_DQN_extentions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IJsTc6AOHBm",
        "colab_type": "text"
      },
      "source": [
        "# 07. DQN Extentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ9yHCbBOHEQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "* The PyTorch Agent Net library\n",
        " - Agent\n",
        " - Agent's experience\n",
        " - Experience buffer\n",
        " - Gym env wrappers\n",
        "* Basic DQN\n",
        "* N-step DQN\n",
        " - Implementation\n",
        "* Double DQN\n",
        " - Implementation\n",
        " - Results\n",
        "* Noisy networks\n",
        " - Implementation\n",
        " - Results\n",
        "* Prioritized replay buffer\n",
        " - Implementation\n",
        " - Results\n",
        "* Dueling DQN\n",
        " - Implementation\n",
        " - Results\n",
        "* Categorical DQN\n",
        " - Implementation\n",
        " - Results\n",
        "* Combining everything\n",
        " - Implementation\n",
        " - Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsyxovmGnoUt",
        "colab_type": "text"
      },
      "source": [
        "N-steps DQN: How to improve convergence speed and stability with a simple unrolling of the Bellman equation and why it's not an ultimate solution\n",
        "\n",
        "Double DQN: How to deal with DQN overestimation of the values of actions\n",
        "\n",
        "Noisy networks: How to make exploration more efficient by adding noise to the network weights\n",
        "\n",
        "Prioritized replay buffer: Why uniform sampling of our experience is not the best way to train\n",
        "\n",
        "Dueling DQN: How to improve convergence speed by making our network's architecture closer represent the problem we're solving\n",
        "\n",
        "Categorical DQN: How to go beyond the single expected value of action and work with full distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL6RoK2SOHHC",
        "colab_type": "text"
      },
      "source": [
        "## The PyTorch Agent Net library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EntU5sn7oBzm",
        "colab_type": "text"
      },
      "source": [
        "PyTorch AgentNet(PTAN) ( https://github.com/yandexdataschool/AgentNet )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwYTGvI2OHJ2",
        "colab_type": "text"
      },
      "source": [
        "## Basic DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNGh24T4sRED",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f1.jpeg\" width=800 /></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q28ayxk3OHMo",
        "colab_type": "text"
      },
      "source": [
        "## N-step DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msETePJGsftI",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f2.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f3.jpg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f4.jpg\" width=800 /></p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SGDU1SMOHPd",
        "colab_type": "text"
      },
      "source": [
        "## Double DQN\n",
        "\n",
        "Double dqn \n",
        "Target Network과 Train Network으로 쪼개어 수행\n",
        "Double Q-Learning은 기존에 존재했던 Q-Learning 개선 알고리즘입니다. DQN의 등장 이후 두 알고리즘이 합쳐져 지금까지도 널리 쓰이고 있는 DDQN으로 발전했습니다.\n",
        "Stochastic한 환경에서 Q-Learning은 종종 좋지 않은 성능을 보이며, 그 원인으로 Overestimation이 오래 전부터 거론되었습니다. 또한 Overestimation의 원인으로는 Q-learning의 Maximum Estimation(ME)이 거론되었는데요.\n",
        "\n",
        "이를 single estimator의 문제점으로 규정하고, 이에 대한 대안으로 double estimator를 제안합니다.\n",
        "\n",
        "[링크 텍스트](https://parkgeonyeong.github.io/Double-DQN%EC%9D%98-%EC%9D%B4%EB%A1%A0%EC%A0%81-%EC%9B%90%EB%A6%AC/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8uzrObys4mQ",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f6.jpeg\" width=800 /></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ds3pqZnOHWI",
        "colab_type": "text"
      },
      "source": [
        "## Noisy DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Y0iaYatGe4",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f7.jpeg\" width=800 /></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4dvem9UOHZi",
        "colab_type": "text"
      },
      "source": [
        "## Prioritized replay buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FpVb1J4tLyq",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f8.jpeg\" width=800 /></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvuripbsQ_qT",
        "colab_type": "text"
      },
      "source": [
        "## Dueling DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dHKgt0itaVF",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f9.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f10.jpeg\" width=800 /></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKDZLJ2QQ_tO",
        "colab_type": "text"
      },
      "source": [
        "## Categorical DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkj7U4qUttKp",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f11.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f12.jpeg\" width=800 /></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Dw08WZQ_v2",
        "colab_type": "text"
      },
      "source": [
        "## Combining everything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd-FuAP7Q_yh",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter7/img/f13.jpeg\" width=800 /></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOP1Okk3h9-_",
        "colab_type": "text"
      },
      "source": [
        "https://openai.com/blog/openai-five/\n",
        "\n",
        "https://arxiv.org/abs/1710.02298"
      ]
    }
  ]
}