{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[RL강의-2]Value Interation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylee33/RL_Lecture/blob/master/%5BRL%EA%B0%95%EC%9D%98_2%5DValue_Interation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eveG1BmU-ujI",
        "colab_type": "text"
      },
      "source": [
        "**Policy Iteration과 다르게 세 번째 iteration(k=3)에서 이미 converge하는 것을 알 수 있습니다. Value Iteration을 구현한 코드를 보겠습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DeFsOLi-zkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIqP7oac-3XH",
        "colab_type": "text"
      },
      "source": [
        "# Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM4C7vis-2NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_state(state, action):\n",
        "    \n",
        "    action_grid = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "    \n",
        "    state[0]+=action_grid[action][0]\n",
        "    state[1]+=action_grid[action][1]\n",
        "    \n",
        "    if state[0] < 0 :\n",
        "        state[0] = 0\n",
        "    elif state[0] > 3 :\n",
        "        state[0] = 3\n",
        "    \n",
        "    if state[1] < 0 :\n",
        "        state[1] = 0\n",
        "    elif state[1] > 3 :\n",
        "        state[1] = 3\n",
        "    \n",
        "    return state[0], state[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_r3yYU_AJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_evaluation(grid_width, grid_height, action, policy, iter_num, reward=-1, dis=1):\n",
        "    \n",
        "    # table initialize\n",
        "    post_value_table = np.zeros([grid_height, grid_width], dtype=float)\n",
        "    \n",
        "    # iteration\n",
        "    if iter_num == 0:\n",
        "        print('Iteration: {} \\n{}\\n'.format(iter_num, post_value_table))\n",
        "        return post_value_table\n",
        "    \n",
        "    for iteration in range(iter_num):\n",
        "        next_value_table = np.zeros([grid_height, grid_width], dtype=float)\n",
        "        for i in range(grid_height):\n",
        "            for j in range(grid_width):\n",
        "                if i == j and ((i == 0) or (i == 3)):\n",
        "                    value_t = 0\n",
        "                else :\n",
        "                    value_t_list= []\n",
        "                    for act in action:\n",
        "                        i_, j_ = get_state([i,j], act)\n",
        "                        value = (reward + dis*post_value_table[i_][j_])\n",
        "                        value_t_list.append(value)\n",
        "                    next_value_table[i][j] = max(value_t_list)\n",
        "        iteration += 1\n",
        "        \n",
        "        # print result\n",
        "        if (iteration % 10) != iter_num: \n",
        "            # print result \n",
        "            if iteration > 100 :\n",
        "                if (iteration % 20) == 0: \n",
        "                    print('Iteration: {} \\n{}\\n'.format(iteration, next_value_table))\n",
        "            else :\n",
        "                if (iteration % 10) == 0:\n",
        "                    print('Iteration: {} \\n{}\\n'.format(iteration, next_value_table))\n",
        "        else :\n",
        "            print('Iteration: {} \\n{}\\n'.format(iteration, next_value_table ))\n",
        "        \n",
        "       \n",
        "        post_value_table = next_value_table\n",
        "        \n",
        "            \n",
        "    return next_value_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xms7o4cJ_CFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_width = 4\n",
        "grid_height = grid_width\n",
        "action = [0, 1, 2, 3] # up, down, left, right\n",
        "policy = np.empty([grid_height, grid_width, len(action)], dtype=float)\n",
        "for i in range(grid_height):\n",
        "    for j in range(grid_width):\n",
        "        for k in range(len(action)):\n",
        "            if i==j and ((i==0) or (i==3)):\n",
        "                policy[i][j]=0.00\n",
        "            else :\n",
        "                policy[i][j]=0.25\n",
        "policy[0][0] = [0] * grid_width\n",
        "policy[3][3] = [0] * grid_width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ItZeLy_GQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "eb2880ff-af77-4755-b46b-c5cd43fb59f5"
      },
      "source": [
        "value = policy_evaluation(grid_width, grid_height, action, policy, 1)\n",
        "value = policy_evaluation(grid_width, grid_height, action, policy, 2)\n",
        "value = policy_evaluation(grid_width, grid_height, action, policy, 3)\n",
        "value = policy_evaluation(grid_width, grid_height, action, policy, 10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1 \n",
            "[[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n",
            "\n",
            "Iteration: 2 \n",
            "[[ 0. -1. -2. -2.]\n",
            " [-1. -2. -2. -2.]\n",
            " [-2. -2. -2. -1.]\n",
            " [-2. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 3 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 10 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3fdnKz8_H96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b634c53-c4e1-4c80-b3e7-83c5d64090fc"
      },
      "source": [
        "value = policy_evaluation(grid_width, grid_height, action, policy, 100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 10 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 20 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 30 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 40 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 50 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 60 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 70 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 80 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 90 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Iteration: 100 \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNtapgYE_QZB",
        "colab_type": "text"
      },
      "source": [
        "여기서 Policy Iteration과 Value Iteration의 눈에 띄는 차이점은 policy improvement가 없다는 것입니다. Policy Iteration의 경우에는 policy에 따라 value func.이 확률적으로 주어지게됩니다. 따라서, 기댓값으로 value func.을 구해야만하고 Bellman expectation eqn.을 이용하게됩니다. \n",
        "\n",
        "하지만 이와달리 Value Iteration의 경우에는 현재 policy가 optimal하다는 것을 전제해서 value func.의 max를 취하기때문에 deterministic한 action이 됩니다.\n",
        "\n",
        "optimal policy에서는 a = argmax q∗(s, a)일때 π∗(a|s) =1 이기때문입니다. 따라서, Value Iteration에서는 이미 value func.의 ture value를 찾는 policy evaluation에 optimal한 policy로의 improvement가 내재되어있습니다. \n",
        "\n",
        "따라서 policy evaluation과정만으로도 optimal policy로 다가갈 수 있습니다. policy evaluation도 결국 여러 iteration을 진행하기에 policy도 그 만큼 update됩니다. 작성한 코드에서도 policy evaluation 함수 내 # update policy부분(policy improvement)이 있는 것도 같은 이유입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhlnfkjc_oum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}